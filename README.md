# Introduction
è¿™æ˜¯ä¸»è¦åŸºäºpythonçš„numpyå®ç°çš„å·ç§¯ç¥ç»ç½‘ç»œ(convolutional neural network)ï¼Œå…·ä½“å†…å®¹å¯æŸ¥çœ‹ä½œä¸šè¯´æ˜PDF~  

æœ¬å·ç§¯ç¥ç»ç½‘ç»œåŸºäº [Mini-SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) + [1/t decay](https://www.jianshu.com/p/d8222a84613c) + [RMSProp](https://zhuanlan.zhihu.com/p/79981927) å®ç°  

å…³äºCNNçš„è¯´æ˜å¯ä»¥å‚è€ƒ[è¿™ç¯‡æ–‡ç« ](https://medium.com/@pkqiang49/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-cnn-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86-%E7%8B%AC%E7%89%B9%E4%BB%B7%E5%80%BC-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8-6047fb2add35)ã€‚è®²çš„å¾ˆå¥½å¾ˆæœ‰æ„æ€ä¹Ÿæ²¡æœ‰æ¶‰åŠå¤ªå¤šæ‹—å£éš¾æ‡‚çš„çŸ¥è¯†ã€‚

## è¿è¡Œæˆªå›¾  

**è¿è¡Œæ—¶çš„æ ·å­**  
<div  align="center">   
    <img src="./è¿è¡Œæ—¶çš„æ ·å­.png" align=center />
</div> 

**Input**: (6+1)ä¸‡å¼ 28x28x1çš„æ‰‹å†™æ•°å­—å›¾ç‰‡
<div  align="center">   
    <img src="./input_ex.png" alt="input image" align=center />
</div>  

**output**: éšæœºé€‰æ‹©çš„num_plotå¼ å¸¦ç€labelçš„è¾“å‡º
<div  align="center">   
    <img src="./output_ex.png" alt="output image" align=center />
</div>

# é¡¹ç›®è¯´æ˜  

**é¦–å…ˆæ„Ÿè°¢ä¸€ä¸ªæœ‹å‹ï¼Œè‡ªå·±çš„ç”µè„‘å¤ªçƒ‚æ‰€ä»¥ç”¨ä»–çš„ç”µè„‘å¸®æˆ‘è·‘ï¼Œå¸®äº†å¾ˆå¤§çš„å¿™ã€‚ æ„Ÿè°¢ğŸ¦€ğŸ¦€ã€‚**  
* æ€»å…±æœ‰ä¸¤ä¸ªpyæ–‡ä»¶ï¼šnn.pyå’Œmnist.py   
    * nn.py è´Ÿè´£å®ç°convolutional_layerå’Œmax_pooling_layerä¸¤ä¸ªlayersçš„æ­£ç¡®ä¸å¦æµ‹è¯•ã€‚  
    * mnist.py è´Ÿè´£å›¾åƒè¯†åˆ«çš„æ•´ä¸ªå·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹æ„å»ºï¼ˆä¸»è¦å†…å®¹åœ¨è¿™ï¼‰ã€‚  
    
    
* æ¨¡å‹è¿è¡Œçš„æµç¨‹  

    **ä¸‹é¢æ˜¯è®¾å®šçš„å„ç§å‚æ•°**  

```
# ä¸€äº›å¼€å…³å‚æ•°
load_para               # Trueè¡¨ç¤ºåŠ è½½å·²æœ‰å·ç§¯æ ¸ï¼ŒFalseè¡¨ç¤ºä½¿ç”¨éšæœºç”Ÿæˆçš„å·ç§¯æ ¸
is_learning             # Trueè¡¨ç¤ºå­¦ä¹ ï¼ŒFalseè¡¨ç¤ºåªæ˜¯æµ‹è¯•å›¾ç‰‡
```  

```
# æ¨¡å‹çš„å‚æ•°
num_plot                # æƒ³æµ‹è¯•çš„å›¾ç‰‡æ•°é‡
sample_index            # batch_sizeå¼ å›¾ä¸­éšæœºé€‰num_plotå¼ 
predicted               # é¢„æµ‹å‡½æ•°å°†ä¼šæ”¾åˆ°è¿™é‡Œ
batch_size              # inputå›¾çš„æ•°é‡
input_size              # å›¾å¤§å°ï¼ˆåƒç´ ï¼‰
in_ch_size              # å›¾çš„depth (grayscale or RGB)
filter_width            # filterçš„size
filter_height = filter_width
num_filters             # filteræ•°
class_num               # classæ•°
num_train               # è®­ç»ƒæ•°
lr                      # learning rate
cnv_lr = lr
fcl_lr = lr
decay                   # 1/t decay
break_threshold         # å½“losså°äºä¸€å®šå€¼æ—¶breakçš„é˜ˆå€¼
M                       # Mini-SGDçš„å‚æ•°
cnvRMS_r_W = 0          # RMSPropçš„å‚æ•°
fclRMS_r_W = 0
alpha
```  
    
 **ä¸‹é¢æ˜¯å­¦ä¹ éƒ¨åˆ†çš„ä»£ç ä»‹ç»**  
```
# convolution layer
    
# é¦–å…ˆå°†å›¾ç‰‡è¾“å…¥åˆ°å·ç§¯å±‚ (convolution layer)
cnv_out = cnv.forward(Xï¼‰

# å·ç§¯å±‚ç»“æŸååˆ°è¾¾æ± åŒ–å±‚ï¼Œè¿™é‡Œç”¨çš„æ˜¯æœ€å¤§æ±  (maxpool)
mpl_out = mpl.forward(cnv_out)

# æ± åŒ–ç»“æŸä¹‹åè¿›å…¥å…¨è¿æ¥å±‚ (fully connect layer)
fcl_out = fcl.forward(mpl_out)  # shape = (batch_size, in_ch_size(class_num), mpl_out_size, ~)

# é€šè¿‡softmaxå’Œcross-entropy è®¡ç®—lossæŸå¤±å‡½æ•°
# softmax layer
smax_in = fcl_out.reshape(batch_size, class_num).T
smax_out = smax.forward(smax_in)    # shape = (class_num, batch_size)

# è®°å½•æŸå¤±å‡½æ•°cent loss
loss_out[ntrain] = cent.forward(smax_out, y)

# å¼€å§‹è®¡ç®—æ¢¯åº¦è¿›è¡Œåå‘ä¼ æ’­
# back smax and cent layer
b_smax_cent_out = cent.backprop(smax_out, y)    # (class_num, batch_size)

# å…¨è¿æ¥å±‚çš„åå‘ä¼ æ’­
# back fully connect layer
b_fcl_in = b_smax_cent_out.T.reshape(batch_size, class_num, 1, 1)
b_fcl_out, b_fcl_out_W, b_fcl_out_b = fcl.backprop(mpl_out, b_fcl_in)

# æ± åŒ–å±‚çš„åå‘ä¼ æ’­
# back max pool layer
b_mpl_out = mpl.backprop(cnv_out, b_fcl_out)

# å·ç§¯å±‚çš„åå‘ä¼ æ’­
# back convolution layer
b_cnv_out, b_cnv_out_W, b_cnv_out_b = cnv.backprop(X, b_mpl_out)

# å°†å¾—åˆ°çš„æ¢¯åº¦è®¡ç®—å¾—åˆ°RMSPropå‚æ•°ï¼Œå¹¶å¯¹å·ç§¯æ ¸Wå’Œbiasè¿›è¡Œæ›´æ–°
# RMSProp
cnvRMS_r_W = (alpha*cnvRMS_r_W) + (1-alpha) * (b_cnv_out_W**2)
cnvRMS_W = (cnv_lr*b_cnv_out_W) / (cnvRMS_r_W**0.5+1e-7)
fclRMS_r_W = (alpha*fclRMS_r_W) + (1-alpha) * (b_fcl_out_W**2)
fclRMS_r = (fcl_lr*b_fcl_out_W) / (fclRMS_r_W**0.5+1e-7)

# update convolution layer
cnv.update_weights(-cnvRMS_W, -b_cnv_out_b*cnv_lr)

# update fully connect layer
fcl.update_weights(-fclRMS_r, -b_fcl_out_b*fcl_lr)

# æ¯æ¬¡è®­ç»ƒç»“æŸåä¼šæ˜¾ç¤ºä¿¡æ¯ï¼Œå¦‚lossï¼Œç¬¬å‡ æ¬¡è®­ç»ƒï¼Œä»¥åŠéƒ¨åˆ†çš„Wå’Œbçš„ä¿¡æ¯ã€‚
# show info
print()
print("[%s]th epoch(s)\nloss: %s" % (ntrain, loss_out[ntrain]))
print("Cnv update: weights = %s, bias = %s" % (b_cnv_out_W[0][0].reshape(filter_width**2)[13:16]*cnv_lr, b_cnv_out_b[0][1:4].T*cnv_lr))
cnv_current_para = cnv.get_weights()
print("Cnv current para: weights =", cnv_current_para[0][0][0].reshape(filter_width**2)[13:16], ", bias =", cnv_current_para[1][0][1:4].T)

# è®¾ç½®lossæ£€æµ‹ï¼Œlosså°äºä¸€å®šå€¼ä¹‹åbreakã€‚
if ntrain > 10:
    if loss_out[ntrain-1]+loss_out[ntrain-2]+loss_out[ntrain-3] < break_threshold:
         break

# æœ€åé€šè¿‡1/t decayå¯¹learning rateè¿›è¡Œè°ƒæ•´
# 1/t decay
cnv_lr = lr * 1.0 / (1.0+decay*ntrain)
fcl_lr = cnv_lr
    
# å­¦ä¹ ç»“æŸï¼Œè¿›è¡Œä¸‹ä¸€æ¬¡çš„å­¦ä¹ ã€‚
```  

**ä¸‹é¢æ˜¯é¢„æµ‹çš„è¿‡ç¨‹**  

```
# å› ä¸ºæ¯æ¬¡åªé¢„æµ‹ä¸€å¼ å›¾æ‰€ä»¥æŠŠbatch_sizeè®¾ç½®æˆ1
batch_size = 1
# å¯¹num_plotå¼ å›¾è¿›è¡Œé¢„æµ‹
for i in range(num_plot):
    # inputæ•°æ®çš„shapeè°ƒæ•´
    pred_cnv_in = X[sample_index[i]].reshape(1, in_ch_size, input_size, input_size)
    
    # è¿›å…¥å·ç§¯å±‚
    pred_cnv_out = cnv.forward(pred_cnv_in)
    
    # æ± åŒ–å±‚
    pred_mpl_out = mpl.forward(pred_cnv_out)

    # å…¨è¿æ¥å±‚
    pred_fcl_out = fcl.forward(pred_mpl_out)
    
    # softmaxå°†ç»“æœæ˜ å°„åˆ°10ä¸ªclassesä¸Šï¼Œæ¦‚ç‡æœ€å¤§çš„å°±æ˜¯é¢„æµ‹çš„class
    pred_smax_in = pred_fcl_out.reshape(batch_size, class_num).T
    pred_smax_out = smax.forward(pred_smax_in)

    # å°†é¢„æµ‹ç»“æœä¿å­˜åˆ°predictedä¸­
    predicted[i] = np.argmax(pred_smax_out)
```  

æœ€åçš„è¯å°†ç»“æœé€šè¿‡pltè¾“å‡ºå¾—åˆ°ä¸Šè¿°çš„outputä¸­çš„æ ·å­ã€‚


## nn.py  
é‡Œé¢æœ‰nn_convolutional_layerå’Œnn_max_pooling_layerä¸¤ä¸ªclassesï¼Œåˆ†åˆ«è´Ÿè´£convolutionå’Œmaxpoolæ“ä½œã€‚  
æ¯ä¸ªclassé‡Œä¸»è¦æœ‰ä¸¤ä¸ªå‡½æ•°ï¼šforwardå’Œbackwardï¼Œforwardè´Ÿè´£æ‰§è¡Œæ“ä½œï¼Œbackwardè´Ÿè´£è®¡ç®—æ¢¯åº¦ã€‚  
å‰©ä½™çš„ä¸»ä½“ä»£ç ä¸»è¦æ˜¯ç”Ÿæˆæ•°æ®æ¥æµ‹è¯•ä¸Šé¢çš„ä¸¤ä¸ªclassesæ˜¯å¦èƒ½æ­£å¸¸ä½¿ç”¨ï¼Œä»¥åŠè°ƒæ•´å„ç§æ•°æ®å(ex: input_size, batch_size, filter_sizeç­‰)ä»£ç æ˜¯å¦ä¾æ—§èƒ½è¿è¡Œç­‰ã€‚  

### nn.pyçš„è¿è¡Œç»“æœ  
#### batch_sizeè®¾ç½®ä¸º8æ—¶  
![image](./nnResult1.png)  
#### batch_sizeè®¾ç½®ä¸º32æ—¶(ç”±äºå¤„ç†æ•°æ®å˜å¤šæ‰€ä»¥è¿è¡Œæ—¶é—´ä¹Ÿå˜å¤šäº†)  
![image](./nnResult2.png)
  
  
## mnist.py  

#### è¿‡ç¨‹æ—¥è®°

##### Nov 24, 2020  
å„ä¸ªè¿æ¥å±‚å¯ä»¥è¿æ¥ï¼Œæ­£å¸¸è°ƒæ•´å‚æ•°(input_size, filter_num, pool_sizeç­‰)ã€‚  
ä½†æ˜¯å¯¹äºæ± åŒ–å±‚åˆ°å…¨è¿æ¥å±‚çš„è¿‡åº¦æœ‰ç‚¹ä¸æ˜¯å¾ˆæ˜ç™½ï¼Œç›®å‰æ˜¯æ± åŒ–å±‚ç»“æŸåå†å°†å…¶ä»¥ä¸€ç»´reshapeã€‚
å¦‚æœ‰4ä¸ªfilterï¼Œæ± åŒ–å±‚ç»“æŸåçš„å¤§å°æ˜¯6-by-6ï¼Œæ‰€ä»¥reshape(4Ã—6Ã—6)ï¼Œç„¶åå†ä¹˜Wå¾—åˆ°10ä¸ªlabelsçš„å€¼ã€‚  
lossæ˜¯ç”¨softmax + cross-entropyæ¥è®¡ç®—çš„ï¼Œç›®å‰é—®é¢˜æ˜¯lossä¼˜åŒ–åˆ°ä¸€å®šæ•°å€¼åå°±ä¸€ç›´åœ¨è¿™ä¸ªæ•°å­—æ³¢åŠ¨äº†ã€‚  
ç”±äºæ•™æˆåªè®²äº†convolution layerå’Œpools layerã€‚åé¢çš„æ“ä½œå…ˆçœ‹çœ‹åˆ«äººçš„æ€è·¯å†è¿›è¡Œã€‚å§‘ä¸”å…ˆä¸Šä¼ ä¿å­˜ä¸‹è¿›åº¦å§ã€‚  
  
##### Nov 25, 2020
æŠŠforwardå’Œbackpropçš„è¿‡ç¨‹ä¿®æ”¹äº†ä¸€ç‚¹ï¼Œç›®å‰å¯ä»¥è®¤å›¾äº†ã€‚  
ä½†æ˜¯ç”µè„‘é…ç½®å¤ªå·®ï¼Œåªæµ‹è¯•äº†å­¦ä¹ 50ï¼Œ100ï¼Œ 500ï¼Œ1000ï¼Œ5000å¼ å›¾çš„æƒ…å†µï¼Œå‡å¯ä»¥æ­£å¸¸è¯†åˆ«å‡ºå›¾ä¸­çš„æ•°å­—ã€‚  
å­¦ä¹ çš„å›¾ç‰‡å†å¤šçš„è¯å¦‚10000å¼ å›¾æ²¡è¯•è¿‡äº†è·‘äº†ä¸€å¤©éƒ½æ²¡è·‘å®Œã€‚ã€‚ã€‚ä½†æ˜¯æ€»å…±æœ‰6ä¸‡å¼ å›¾ï¼Œç»æœ›ã€‚

##### Nov 27, 2020
æ˜¨æ™šæŠŠä»£ç ä¼˜åŒ–äº†ï¼Œä¹‹å‰ç”¨çš„æ˜¯GDï¼Œç›®å‰ç‰ˆæœ¬æ˜¯åŸºäºmini-SGD + RMSProp + 1/t decayå®ç°ï¼Œ  
é€Ÿåº¦å¤§å¤§åŠ å¿«ä¸€æ™šä¸Šå°±å­¦å®Œ60000å¼ å­¦ä¹ èµ„æ–™å¹¶ä¿å­˜äº†å·ç§¯æ ¸ã€‚
ç°åœ¨å·²ç»å¯ä»¥æ‹¿æ¥ç”¨äº†ï¼Œå­¦ä¹ æ—¶é—´æœ‰é™+é˜²æ­¢overfittingï¼Œ  
ç›®å‰æŠŠlossè°ƒæ•´åˆ°ä¸‰æ¬¡çš„å’Œä½äº500000å°±é€€å‡ºï¼Œæ‰€ä»¥æ­£ç¡®ç‡ä¸æ˜¯100%ä½†æ˜¯é™¤äº†æ¯”è¾ƒåƒçš„å›¾ç‰‡ä¸€èˆ¬éƒ½èƒ½å¯¹äº†ã€‚  
åç»­è¿˜åœ¨ä¼˜åŒ–ä¸­ï¼Œä¸è¿‡å·®ä¸å¤šèƒ½æäº¤äº†ã€‚  
è°èƒ½æƒ³åˆ°20åˆ†çš„é¡¹ç›®ä½œä¸šæ¢¯åº¦å‡½æ•°å’Œå»ºç«‹æ¨¡å‹17åˆ†ï¼Œåªå 3åˆ†çš„æ­£ç¡®è¯†åˆ«å›¾ç‰‡å´èŠ±äº†80%çš„æ—¶é—´å’Œç²¾åŠ›ã€‚ğŸ˜“